{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing attention briefly\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Study examples of attention\n",
    "\n",
    "<img src=\"attention_ex1.PNG\" width=\"800\" align=\"center\"/> \n",
    "\n",
    "<img src=\"attention_ex2.PNG\" width=\"800\" align=\"center\"/> \n",
    "\n",
    "<img src=\"attention_ex3.PNG\" width=\"1000\" align=\"center\"/> \n",
    "\n",
    "***\n",
    "\n",
    "<span style=\"font-size:0.5em;\"> \n",
    "[참고] A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING  \n",
    "       Show, Attend and Tell: Neural Image Caption Generation with Visual Attention </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence2sequence problems\n",
    "ex) 번역, 챗팅, 문서, 이미지 캡션, ..., (주소 --> 우편번호)\n",
    "\n",
    "#### Encoder-Decoder model\n",
    "Seq2seq 문제 해결에 효과적이며 널리 쓰임\n",
    "\n",
    "#### Structures of Attention in encoder-decoder\n",
    "0. RNN wo attention\n",
    "1. RNN + attention\n",
    "2. Attention wo RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple encoder-decoder model\n",
    "<img src=\"encoder_decoder.PNG\" width=\"800\" align=\"center\"/> \n",
    "* 두 개의 RNN(encoder, decoder) 이 연결된 형태 : SimpleRNN, LSTM, GRU\n",
    "    * input : (batch_size, timesteps, input_dim)\n",
    "    * output : \n",
    "        * (batch_size, timesteps, units) if return_sequences\n",
    "        * (batch_size, units) if return_state\n",
    "* 예) 뒷 단어 Y = f(W, X, (A, B, C))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple encoder-decoder with Sequential\n",
    "\n",
    "Sequential로 모델을 구성하는 경우 encoder의 출력 값을 decoder 의 입력값에 맞춰주어야 함\n",
    "\n",
    "```python\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "model.add(RepeatVector(n_timesteps_in))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "```\n",
    "\n",
    "[참고] https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple encoder-decoder with Model\n",
    "\n",
    "Model 로 모델을 구성하는 경우 encoder의 최종 상태를 decoder 의 시작 상태로 넣어 줌\n",
    "```python\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "```\n",
    "[참고] https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another encoder-Decoder model\n",
    "<img src=\"encoder_decoder2.jpg\" width=\"600\" align=\"left\"/>  \n",
    "\n",
    "* Encoder로 얻은 context를 decoder 초기값과 이후 시점 계산에도 더해줌\n",
    "    * decoder 진행에 따른 처음 정보의 손실 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention model\n",
    "\n",
    "#### 기존 모델 단점 :\n",
    "* context 로 sentence의 모든 정보 집약 후 이로부터 새롭게 문장 생성\n",
    "    * 여러 단어들의 조합인 문장이 단어 하나와 동일한 크기의 벡터 내에 저장될 수 있을까?  \n",
    "    * sentence1 | encoder | --> context --> | decoder | sentence1p\n",
    "*  문장이 길어질수록 성능 나빠짐  \n",
    "\n",
    "#### Attention 추가를 통한 보완\n",
    "* (기존) 한 개 context vector와 연결 --> 전체 입력 hidden state 와 연결\n",
    "* <span style=\"color:red\"> decoder 각 요소와 encoder 각 요소간 연관성 확인 가능</span>\n",
    "\n",
    "<img src=\"encoder_decoder3.jpg\" width=\"400\" align=\"left\"/>  \n",
    "\n",
    "\n",
    "언어모델 : 현재까지의 문장으로부터 다음 단어 예측,  \n",
    "$ p(y_i|y_1, ..., y_{i-1}, x)$   \n",
    "<br>\n",
    "$ p(y_i|y_1, ..., y_{i-1}, x) = g(y_{i-1}, s_i, c_i) $  \n",
    "$ s_i = f(s_{i-1}, y_{i-1}, c_i) $  \n",
    "$ \\hspace{0.7cm} c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j $  \n",
    "$ \\hspace{1.4cm} \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} $  \n",
    "$ \\hspace{1.4cm} e_{ij} = a(s_{i-1}, h_j) = v_a^T \\tanh(W_a s_{i-1} + U_a h_j) $  \n",
    "$ g, f, a $ 는 데이터로부터 학습할 함수, 사용자가 설정 가능     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention 예시\n",
    "\n",
    "#### Toy example\n",
    "x1, x2, x3 로 구성된 sequence 가 있는 경우   \n",
    "\n",
    "```python  \n",
    "h1, h2, h3 = Encoder(x1, x2, x3)\n",
    "\n",
    "# a는 데이터로 부터 학습할 함수, 예) feed forward network\n",
    "e11 = a(0, h1) # h : encoder 중간 상태\n",
    "e12 = a(0, h2)\n",
    "e13 = a(0, h3)\n",
    "\n",
    "e21 = a(s1, h1) # s : decoder 중간 상태\n",
    "e22 = a(s1, h2)\n",
    "e23 = a(s1, h3)\n",
    "\n",
    "alpha11 = exp(e11) / (exp(e11) + exp(e12) + exp(e13))\n",
    "alpha12 = exp(e12) / (exp(e11) + exp(e12) + exp(e13))\n",
    "alpha13 = exp(e13) / (exp(e11) + exp(e12) + exp(e13))\n",
    "\n",
    "alpha21 = exp(e21) / (exp(e21) + exp(e22) + exp(e23))\n",
    "alpha22 = exp(e22) / (exp(e21) + exp(e22) + exp(e23))\n",
    "alpha23 = exp(e23) / (exp(e21) + exp(e22) + exp(e23))\n",
    "\n",
    "c1 = alpha11 * h1 + alpha12 * h2 + alpha13 * h3\n",
    "c2 = alpha21 * h1 + alpha22 * h2 + alpha23 * h3\n",
    "\n",
    "s1 = Decoder(c1)\n",
    "s2 = Decoder(s1, y1, c2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention 예시 (계속)\n",
    "\n",
    "#### RNN vs Attention\n",
    "* 문제 : 랜던한 수열에 대한 학습\n",
    "    * [1,3,5,2,4] --> [1, 3]\n",
    "    * 랜덤 수 이므로 실제 앞 뒤 숫자간 규칙 없음 : RNN은 찾기 어려움\n",
    "    * Attention 은 입력, 출력의 관련성을 확인하므로 복사되는것 알아냄  \n",
    "* [attention 구현 코드](attention_layer.ipynb) : https://github.com/datalogue/keras-attention  \n",
    "\n",
    "#### attention layer 적용\n",
    "\n",
    "```python\n",
    "# define the encoder-decoder with attention model\n",
    "def attention_model(n_timesteps_in, n_features):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "\tmodel.add(AttentionDecoder(150, n_features))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\treturn model\n",
    "```\n",
    "\n",
    "#### attention 정의 함수의 호출 구문 (step_function)\n",
    "\n",
    "```python\n",
    "# https://keras.io/backend/\n",
    "keras.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)\n",
    "# Iterates over the time dimension of a tensor.\n",
    "...\n",
    "```\n",
    "\n",
    "#### attention 정의 함수\n",
    "\n",
    "```python\n",
    "# in class AttentionDecoder\n",
    "def step(self, x, states):\n",
    "    ytm, stm = states\n",
    "\n",
    "    # repeat the hidden state to the length of the sequence\n",
    "    _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "    # now multiplty the weight matrix with the repeated hidden state\n",
    "    _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "    # calculate the attention probabilities\n",
    "    # this relates how much other timesteps contributed to this one.\n",
    "    et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "               K.expand_dims(self.V_a))\n",
    "    at = K.exp(et)\n",
    "    at_sum = K.sum(at, axis=1)\n",
    "    at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "    at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "    # calculate the context vector\n",
    "    context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "[참고]  \n",
    "https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/  \n",
    "https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer : Advanced attention model\n",
    "\n",
    "위의 encoder-decoder 구조에서 RNN 없이 attention 만으로 구성\n",
    "\n",
    "#### RNN 단점\n",
    "* 속도 저하\n",
    "    * 한 단어씩 차례대로 진행함에 따라 병렬 처리 어려움\n",
    "* 문장이 길어질수록 성능 저하\n",
    "\n",
    "#### Transformer 특징 \n",
    "* 각 단어에 대해 별개로 처리하므로 병렬 처리 가능\n",
    "* attention을 이용해 긴 문장에 대해서도 주변 단어들에 대한 영향 반영하여 성능 향상  \n",
    "* __<span style=\"color:red\"> Multi-head attention </span>__  \n",
    "\n",
    "<br>  \n",
    "참고  \n",
    "* https://arxiv.org/abs/1706.03762 (Attention is all you need)  \n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html (코드, pytorch 이나 이해하기 쉬움)  \n",
    "* https://github.com/Separius/BERT-keras/tree/master/transformer (코드)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention in Transformer\n",
    "\n",
    "* attention 일반화 : (기존) s, h --> Q, K, V 로 이해\n",
    "    * 의미 : Query가 주어졌을 때 Key와 비교해 연관성 확인 후 해당 Value에 가중치\n",
    "* 예) 영문 --> 한글문으로 번역하는 경우\n",
    "    * I love you --> 나는 너를 사랑해\n",
    "        * (I love you, 나는 너를) --> (사랑해)\n",
    "        * query : 나는 너를\n",
    "        * key, value : I love you\n",
    "        * \"사랑해\"는 (key, value) 에 대한 query 의 관련성으로 부터 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer 구조\n",
    "<img src=\"transformer1.jpg\" width=\"700\" align=\"center\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder, decoder in Transformer\n",
    "\n",
    "#### 구성 \n",
    "* (embedding, positional encoding, attention, normalization, feed foward) 반복\n",
    "\n",
    "#### encoder, decoder 차이\n",
    "* encoder : self-attention (input 문장만 사용)\n",
    "* decoder : (masked) self-attention, encoder-decoder attention\n",
    "* 예) 영문 --> 한글문\n",
    "    * encoder : 영문에 대한 self-attention \n",
    "    * decoder : \n",
    "        * 한글문 output 앞부분에 대한 self-attention --> query\n",
    "            * mask 를 적용해 각 예측 단어보다 먼저 나온 단어들만 실제 계산에 참여\n",
    "        * 영문 encoder 결과를 (key, value) 로 확인해 처음 영문 및 앞단의 한글 영향을 고려하여(query, key, value) 다음 한글 단어 예측\n",
    "> mask  \n",
    "      * 번역되는 문장은 한 단어씩 차례로 생성하여 전체 문장 완성   \n",
    "          * I love you(0) : 나는(1) --> 너를(2) --> 사랑해(3)\n",
    "      * 아직까지 나오지 않은 단어에 대해서는 계산에 포함하지 않음\n",
    "      * 계산 병렬적으로 적용되어 계산 효율적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder, (decoder) in Transformer (계속)\n",
    "\n",
    "#### Encoder 사용법 및 구현 부분\n",
    "\n",
    "```python\n",
    "x = embedding_layer(inputs)\n",
    "for i in range(num_layers):\n",
    "    x = EncoderLayer(embedding_dim, num_heads, d_hid, residual_dropout,\n",
    "                     attention_dropout, use_attn_mask, i, neg_inf, layer_norm_epsilon, accurate_gelu)(x, attn_mask)\n",
    "return keras.Model(inputs=inputs, outputs=[x], name='Transformer')\n",
    "\n",
    "class EncoderLayer:\n",
    "    def __call__(self, x, mask):\n",
    "        a = self.attention(x, mask)\n",
    "        n = self.ln1(self.add1([x, self.drop1(a)]))\n",
    "        f = self.ffn(n)\n",
    "        return self.ln2(self.add2([n, self.drop2(f)]))\n",
    "```      \n",
    "\n",
    "https://github.com/Separius/BERT-keras/blob/master/transformer/model.py    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-head attention\n",
    "\n",
    "<img src=\"multi_head_ex.PNG\" width=\"800\" align=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-head attention(계속)\n",
    "\n",
    "<img src=\"transformer2_3.PNG\" width=\"600\" align=\"center\"/>  \n",
    "\n",
    "$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}V) $  \n",
    "위에서의 $a$ 함수에 대하여 내적함수 형태로 단순하게 적용\n",
    "대신 CNN 에서 다수의 filter 를 적용하는 것처럼, 여러 개로 나누어 적용 \n",
    "* 각 Q, K, V 데이터를 n_head 개 만큼 split\n",
    "* 각 데이터에 선형 transformation 적용\n",
    "* 내적 적용\n",
    "* 값들을 concatenate 해서 split 되기 이전으로 사이즈로 복원\n",
    "\n",
    "```python\n",
    "def multihead_attention(x, attn_mask, n_head: int, n_state: int, attention_dropout: float, neg_inf: float):\n",
    "    _q, _k, _v = x[:, :, :n_state], x[:, :, n_state:2 * n_state], x[:, :, -n_state:]\n",
    "    # 분리\n",
    "    q = split_heads(_q, n_head)  # B, H, L, C//H\n",
    "    k = split_heads(_k, n_head, k=True)  # B, H, C//H, L\n",
    "    v = split_heads(_v, n_head)  # B, H, L, C//H\n",
    "    a = scaled_dot_product_attention(q, k, v, attn_mask, attention_dropout, neg_inf)\n",
    "    return merge_heads(a)\n",
    "\n",
    "# 내적 부분만 살펴보면\n",
    "def scaled_dot_product_attention(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n",
    "    w = K.batch_dot(q, k)  # w is B, H, L, L\n",
    "    w = w / K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n",
    "    if attn_mask is not None:\n",
    "        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n",
    "    w = K.softmax(w)\n",
    "    w = Dropout(attention_dropout)(w)\n",
    "    return K.batch_dot(w, v)  # it is B, H, L, C//H [like v]\n",
    "```\n",
    "\n",
    "https://github.com/Separius/BERT-keras/blob/master/transformer/funcs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 참고하면 좋을 것\n",
    "1. Keras code : https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py  \n",
    "2. Attention is all you need : https://arxiv.org/pdf/1706.03762.pdf  \n",
    "3. 좋은 설명 : https://jalammar.github.io/illustrated-transformer/  \n",
    "4. 뉴스 분류 응용 : [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
