{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, math, operator\n",
    "import numpy as np, copy as cp\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dot\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "from kutils.file import dump as kdump\n",
    "from kutils.file import load as kload\n",
    "from kutils.file import get_files\n",
    "\n",
    "sys.path.append('/home/bwlee2/research/embedding/word2vec/cbow_update')\n",
    "#sys.path.append(os.getcwd())\n",
    "from word_index import Word_dic\n",
    "from cbow import Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "f2_s = '/home/bwlee2/work/projects/market_sensing/dict/cbow_update/texts.pk'\n",
    "_, _, texts = kload(f2_s)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "N_WINDOW = 3\n",
    "N_NEGATIVE = 5\n",
    "\n",
    "texts1 = texts[:1000]\n",
    "print(type(texts1), type(texts1[0]), type(texts1[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KEmbedding(Cbow):\n",
    "    def __init__(self, n_window, n_negative, embed_dim):\n",
    "        self.n_window, self.n_negative, self.embed_dim = n_window, n_negative, embed_dim\n",
    "        self.word_dic = None\n",
    "\n",
    "    def _concat_tuple(texts):\n",
    "        \"\"\"\n",
    "        if tuple of texts(array of array of string) is given,\n",
    "        it concatenates all tuple in texts format\n",
    "        \"\"\"\n",
    "        temp = []\n",
    "        for text1 in texts:\n",
    "            temp += text1\n",
    "        texts = temp\n",
    "        return texts\n",
    "        \n",
    "    def load_text(self, *texts):\n",
    "        \"\"\"\n",
    "        initialize dictionary from texts\n",
    "        \"\"\"\n",
    "        texts = KEmbedding._concat_tuple(texts)\n",
    "        self.word_dic = Word_dic(texts)\n",
    "        Cbow.__init__(self, self.n_window, self.n_negative, self.embed_dim, self.word_dic)\n",
    "        self.texts = texts\n",
    "        \n",
    "    def add_text(self, *texts):\n",
    "        \"\"\"\n",
    "        preserve self.word_dic and add words after that\n",
    "        \"\"\"\n",
    "        if self.word_dic is None:\n",
    "            self.load_text(*texts)\n",
    "            self.get_network()\n",
    "        else:\n",
    "            texts = KEmbedding._concat_tuple(texts)\n",
    "            \n",
    "            mat_old = self.get_embed()\n",
    "            n_words_old = self.n_words\n",
    "            word_dic_old = cp.copy(self.word_dic)\n",
    "            self.new_words = self.word_dic.update(texts)\n",
    "            \n",
    "            Cbow.__init__(self, self.n_window, self.n_negative, self.embed_dim, self.word_dic)\n",
    "            n_words_new = self.n_words\n",
    "            word_dic_new = self.word_dic\n",
    "\n",
    "            new_word_context_dic = { word1: self._contexts_of(word1, texts) for word1 in self.new_words }\n",
    "            if n_words_old == n_words_new:\n",
    "                mat_new = mat_old\n",
    "            else:\n",
    "                mat_new = []\n",
    "                for ix1 in range(n_words_old, n_words_new):\n",
    "                    word2 = self.word_dic.ix2word[ix1]\n",
    "                    embed2 = self._get_avg_embed(word2, mat_old, word_dic_old, new_word_context_dic)\n",
    "                    mat_new.append(embed2)\n",
    "                mat_new = np.array(mat_new)\n",
    "                mat_new = np.concatenate((mat_old, mat_new))        \n",
    "            \n",
    "            self.set_embed(mat_new)\n",
    "            self.texts = texts\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return super().get_train_data(self.texts)\n",
    "    \n",
    "    def clone(self):\n",
    "        return cp.copy(self)\n",
    "    \n",
    "    def _contexts_of(self, word1a, texts2, n_window=3):\n",
    "        \"\"\"\n",
    "        get top frequent context of word1a from texts2\n",
    "        \"\"\"\n",
    "        context = {}\n",
    "        def add_count_dic(count_dic1, key_arr1):\n",
    "            for key1 in key_arr1:\n",
    "                count_dic1.setdefault(key1, 0)\n",
    "                count_dic1[key1] += 1\n",
    "        def get_top_words(count_dic1):\n",
    "            sorted_count_dic1 = sorted(count_dic1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            n_words = len(sorted_count_dic1)\n",
    "            n_words2 = max(10, int(n_words/10)) # consider 10% of words in calculation\n",
    "            return { word1: count1 for word1, count1 in sorted_count_dic1[:n_words2] }\n",
    "\n",
    "        for text1 in texts2:\n",
    "            n_text1 = len(text1)\n",
    "            for i, word1 in enumerate(text1):\n",
    "                if word1a == word1:\n",
    "                    low_limit = max([i-n_window, 0])\n",
    "                    high_limit = min([i+n_window+1, n_text1])\n",
    "                    arr_temp = text1[low_limit:i]\n",
    "                    arr_temp += text1[i+1:high_limit]\n",
    "                    add_count_dic(context, arr_temp)\n",
    "        return get_top_words(context)\n",
    "\n",
    "    def _get_avg_embed(self, word0, embed_mat1, word_dic1, word_context_dic):\n",
    "        \"\"\"\n",
    "        get embedding of oov word0 by averaging its already seen context\n",
    "        \"\"\"\n",
    "        context_count1 = word_context_dic[word0]        \n",
    "        words = word_dic1.words\n",
    "        total_count_sum = 0\n",
    "        embed = np.zeros(self.embed_dim)\n",
    "        for word1, count1 in context_count1.items():\n",
    "            if word1 in words:\n",
    "                ix1 = word_dic1.word2ix[word1]\n",
    "                embed += embed_mat1[ix1] * count1\n",
    "                total_count_sum += count1\n",
    "        if total_count_sum < 1:\n",
    "            return np.zeros(self.embed_dim)\n",
    "        return embed/total_count_sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embed1 = KEmbedding(N_WINDOW, N_NEGATIVE, EMBEDDING_DIM)\n",
    "embed1.load_text(texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- <keras.engine.training.Model object at 0x7fd9938354a8>\n",
      "Epoch 1/1\n",
      "304340/304340 [==============================] - 16s 51us/step - loss: 0.5296 - out1_loss: 0.1851 - lambda_32_loss: 0.3445\n"
     ]
    }
   ],
   "source": [
    "input1, target1 = embed1.get_train_data()\n",
    "model1 = embed1.get_network()\n",
    "\n",
    "print('--------', embed1.network)\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "score1 = model1.fit(x=input1, y=target1, batch_size=100, epochs=1)\n",
    "mat1 = embed1.get_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111 <keras.engine.training.Model object at 0x7fd9938354a8>\n",
      "2222222 <keras.engine.training.Model object at 0x7fd9938354a8>\n",
      "<keras.engine.training.Model object at 0x7fd9938354a8>\n",
      "new~~~~~~~` ['graphics' 'jpeg' 'gif' ... 'blues' 'duck' 'vuw']\n"
     ]
    }
   ],
   "source": [
    "texts2 = texts[1000:1500]\n",
    "print('111111', embed1.network)\n",
    "#embed2 = cp.copy(embed1)\n",
    "embed2 = embed1.clone()\n",
    "print('2222222', embed1.network)\n",
    "print(embed2.network)\n",
    "embed2.add_text(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9583 12260\n"
     ]
    }
   ],
   "source": [
    "n_words1 = embed1.n_words\n",
    "n_words2 = embed2.n_words\n",
    "print(n_words1, n_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "104482/104482 [==============================] - 7s 66us/step - loss: 1.4391 - out1_loss: 0.7307 - lambda_34_loss: 0.7083\n"
     ]
    }
   ],
   "source": [
    "input2, target2 = embed2.get_train_data()\n",
    "model2 = embed2.get_network()\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "score2 = model2.fit(x=input2, y=target2, batch_size=100, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directly\n",
      "trouble\n"
     ]
    }
   ],
   "source": [
    "print( embed1.word_dic.ix2word[950] )\n",
    "print( embed1.word_dic.ix2word[1950] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03662822  0.08640641  0.09149102 -0.12910344  0.07835306  0.10650671\n",
      "  0.05641901 -0.07046448  0.11508797 -0.12533854 -0.05059731  0.08084092\n",
      "  0.03880278  0.10370369 -0.06310498 -0.06693341  0.11772046 -0.01862401\n",
      "  0.07547874 -0.10211251 -0.07356763 -0.11647589  0.08149291  0.10060213\n",
      " -0.12544914  0.09936368 -0.1133073   0.0321604   0.04843689  0.06579549\n",
      "  0.0731416  -0.10536198 -0.08858644 -0.1032197   0.03922247  0.12872247\n",
      "  0.05878539  0.08730777 -0.05282257  0.05589495 -0.10633729 -0.07055101\n",
      "  0.13383353  0.07207597 -0.13517264 -0.09601953 -0.09697238 -0.03642629\n",
      " -0.10364527  0.01997281 -0.06686973  0.13869137  0.09556331 -0.11943659\n",
      " -0.12136271 -0.12421506  0.14105746 -0.1033272   0.08189443  0.06742942\n",
      " -0.10946915  0.04581811  0.15215345  0.12930353  0.10357404  0.06815642\n",
      "  0.04857005 -0.13451771  0.12757763 -0.13269976  0.08387921 -0.05071742\n",
      " -0.07927927 -0.06023665  0.04752771 -0.07053507  0.12513182 -0.0772993\n",
      " -0.03863328  0.08223251 -0.08222835  0.12718932  0.14476809 -0.03029346\n",
      " -0.14035583  0.06862248 -0.0919495   0.11813662 -0.04400362 -0.13254175\n",
      "  0.08833565 -0.07575583  0.12427375  0.09911004 -0.06295986  0.04317253\n",
      "  0.14247514 -0.09559961 -0.07660861  0.05582908]\n",
      "[-0.0438202   0.01426474  0.01718245 -0.04972886  0.00375277  0.03341588\n",
      " -0.02112741  0.00535508  0.03600632 -0.05550677  0.02154836  0.0040849\n",
      " -0.04204695  0.03227846  0.00679574  0.00512452  0.04276413  0.06209526\n",
      "  0.00110943 -0.03104866  0.00166056 -0.04811052  0.00412634  0.02855998\n",
      " -0.04672134  0.02526358 -0.039883   -0.04224839 -0.02869201 -0.00740871\n",
      " -0.00144012 -0.01993366 -0.01977934 -0.02559538 -0.04046486  0.04924458\n",
      " -0.01278068  0.01081647  0.01647571 -0.01622804 -0.03844002  0.0048124\n",
      "  0.05954905 -0.00775748 -0.05439119 -0.02800311 -0.02250144  0.04004659\n",
      " -0.02342468 -0.05912324  0.01199101  0.05963892  0.02146049 -0.04548601\n",
      " -0.04101916 -0.04679965  0.06535764 -0.03471225  0.00231184 -0.00992633\n",
      " -0.03354096 -0.02932663  0.07474125  0.05395187  0.02288903 -0.00843237\n",
      " -0.02422814 -0.06044198  0.05046679 -0.05354252  0.01058782  0.02200869\n",
      " -0.01482809  0.0190398  -0.02132947  0.00416207  0.05286665 -0.00201805\n",
      "  0.03586607  0.00441785 -0.01067227  0.04681689  0.07372357  0.05884487\n",
      " -0.06601137 -0.01284434 -0.01682317  0.0428717   0.03581813 -0.06664419\n",
      "  0.01692429 -0.00179653  0.03780781  0.02516739  0.00834441 -0.03163635\n",
      "  0.06676684 -0.02411192 -0.00223755 -0.02251907]\n",
      "[ 0.15216379  0.11826377  0.16924866 -0.12387401  0.10665809  0.11118595\n",
      "  0.09584625 -0.14733952  0.11440425 -0.12008378 -0.15412629  0.10696726\n",
      "  0.19279136  0.14870648 -0.13479042 -0.10427158  0.08999127 -0.15968142\n",
      "  0.07702032 -0.09053329 -0.09368581 -0.08228122  0.06453216  0.1388029\n",
      " -0.08236989  0.17530543 -0.17640717  0.14045545  0.10640641  0.10133678\n",
      "  0.11697019 -0.11777592 -0.08174334 -0.10363635  0.08299804  0.1385781\n",
      "  0.16301787  0.14755848 -0.13273335  0.17865062 -0.18985848 -0.13430089\n",
      "  0.16925105  0.12380018 -0.10240966 -0.14547808 -0.10191316 -0.1606051\n",
      " -0.12396068  0.15994489 -0.12914813  0.09867157  0.08896952 -0.08235\n",
      " -0.15392621 -0.15610535  0.16298047 -0.06776598  0.17232199  0.14485458\n",
      " -0.1927352   0.15843466  0.16688214  0.15432984  0.10193042  0.1673699\n",
      "  0.1637807  -0.123868    0.17427239 -0.13941754  0.11761639 -0.13933448\n",
      " -0.10373413 -0.10669779  0.17490967 -0.18194042  0.09404998 -0.1398879\n",
      " -0.1509861   0.07901818 -0.1486497   0.078797    0.13426122 -0.08221369\n",
      " -0.09616458  0.16150396 -0.13011678  0.15549105 -0.10901304 -0.18616225\n",
      "  0.12806696 -0.09558658  0.17191789  0.13923624 -0.10243125  0.09494989\n",
      "  0.10795431 -0.18237032 -0.08757382  0.16973843]\n"
     ]
    }
   ],
   "source": [
    "print(embed1.get_embed('directly'))\n",
    "print(embed2.get_embed('directly'))\n",
    "print(embed2.get_embed('trouble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrated\n",
      "stable\n",
      "displays\n",
      "1930's\n",
      "astray\n",
      "en\n",
      "warfare\n",
      "worm\n",
      "transportation\n",
      "shifting\n",
      "------------------\n",
      "wpd\n",
      "knowing\n",
      "improve\n",
      "rusnews\n",
      "raise\n",
      "lunatic\n",
      "bodies\n",
      "whatsoever\n",
      "viability\n",
      "des\n",
      "------------------\n",
      "connecting\n",
      "accomplish\n",
      "270\n",
      "thereby\n",
      "6th\n",
      "mock\n",
      "medicine\n",
      "357\n",
      "floor\n",
      "infants\n"
     ]
    }
   ],
   "source": [
    "import nmslib\n",
    "\n",
    "mat1 = embed1.get_embed()\n",
    "mat2 = embed2.get_embed()\n",
    "\n",
    "# initialize a new index, using a HNSW index on Cosine Similarity\n",
    "index1 = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index1.addDataPointBatch(mat1)\n",
    "index1.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "index2 = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index2.addDataPointBatch(mat2)\n",
    "index2.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "# query for the nearest neighbours of the first datapoint\n",
    "word1 = 'directly'\n",
    "word2 = 'trouble'\n",
    "\n",
    "ids1, distances1 = index1.knnQuery(mat1[embed1.word_dic.word2ix[word1]], k=10)\n",
    "for id1 in ids1:\n",
    "    print(embed1.word_dic.ix2word[id1])\n",
    "\n",
    "ids2a, distances2a = index2.knnQuery(mat2[embed2.word_dic.word2ix[word1]], k=10)\n",
    "ids2b, distances2b = index2.knnQuery(mat2[embed2.word_dic.word2ix[word2]], k=10)\n",
    "print('------------------')\n",
    "for id1 in ids2a:\n",
    "    print(embed2.word_dic.ix2word[id1])\n",
    "print('------------------')\n",
    "for id1 in ids2b:\n",
    "    print(embed2.word_dic.ix2word[id1])\n",
    "\n",
    "# get all nearest neighbours for all the datapoint\n",
    "# using a pool of 4 threads to compute\n",
    "#neighbours = index.knnQueryBatch(data, k=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "304340/304340 [==============================] - 15s 51us/step - loss: 0.2555 - out1_loss: 0.0128 - lambda_2_loss: 0.2427\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model1 = cbow1.get_network()\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "#score1 = model1.fit(x=[cbow1.data_x, cbow1.data_context, cbow1.data_negative], y=[cbow1.target_data, cbow1.target_negative], batch_size=100, epochs= 1)\n",
    "score1 = model1.fit(x=input1, y=target1, batch_size=100, epochs= 1)\n",
    "mat1 = cbow1.get_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<START>', '<UNK>', 'the', 'of']\n"
     ]
    }
   ],
   "source": [
    "print([ word_dic1.ix2word[i] for i in range(5) ])\n",
    "#mat1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words1 = word_dic1.n_words\n",
    "texts2 = texts[1000:2500]\n",
    "word_dic2 = cp.copy(word_dic1)\n",
    "new_words = word_dic2.update(texts2)\n",
    "n_words2 = word_dic2.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows\n",
      "graphics\n",
      "jpeg\n",
      "gif\n",
      "package\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    word1 = word_dic2.ix2word[i+n_words1]\n",
    "    print(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts_of(word1a, texts2, n_window=3):\n",
    "    context = {}\n",
    "    def add_count_dic(count_dic1, key_arr1):\n",
    "        for key1 in key_arr1:\n",
    "            count_dic1.setdefault(key1, 0)\n",
    "            count_dic1[key1] += 1\n",
    "    def get_top_words(count_dic1):\n",
    "        sorted_count_dic1 = sorted(count_dic1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        n_words = len(sorted_count_dic1)\n",
    "        n_words2 = max(10, int(n_words/10)) # consider 10% of words in calculation\n",
    "        return { word1: count1 for word1, count1 in sorted_count_dic1[:n_words2] }\n",
    "        \n",
    "    for text1 in texts2:\n",
    "        n_text1 = len(text1)\n",
    "        for i, word1 in enumerate(text1):\n",
    "            if word1a == word1:\n",
    "                low_limit = max([i-n_window, 0])\n",
    "                high_limit = min([i+n_window+1, n_text1])\n",
    "                arr_temp = text1[low_limit:i]\n",
    "                arr_temp += text1[i+1:high_limit]\n",
    "                add_count_dic(context, arr_temp)\n",
    "    return get_top_words(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = contexts_of('windows', texts2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 273, 'for': 183, 'and': 181, 'a': 175, 'i': 163, '3': 143, 'in': 135, 'to': 128, '1': 119, 'of': 105, 'is': 102, 'that': 91, 'dos': 90, 'under': 72, 'x': 71, 'it': 67, 'with': 65, 'ms': 60, '2': 60, 'on': 59, 'run': 50, 'or': 50, 'you': 47, 'from': 47, 'os': 45, 'will': 45, 'version': 44, 'nt': 44, 'my': 43, 'but': 40, 'using': 38, 'apps': 36, 'does': 35, '0': 35, 'if': 33, 'not': 32, 'windows': 32, 'when': 31, 'driver': 30, 'this': 30, 'use': 30, 'running': 29, 'have': 29, 'are': 26, 'has': 26, 'be': 26, 'would': 25, 'c': 25, 'as': 24, 'only': 24, 'file': 24, 'drivers': 21, 'microsoft': 21, 'up': 21, 'there': 21, 'do': 20, 'system': 20, 'comp': 20, 'into': 20, 'which': 20, 'what': 18, 'program': 18, 'about': 18, 'just': 17, 'mac': 17, 'was': 17, 'work': 17, 'can': 17, 'all': 17, 'directory': 17, 'word': 17, 'than': 16, 'so': 15, 'programs': 15, 'within': 15, 'time': 15, 'fine': 14, 'linux': 14, 'new': 14, 'like': 14, 'at': 13, 'thanks': 13, 'am': 13, 'an': 13, 'set': 13, 'more': 13, 'then': 13, '4': 13, 'without': 12, 'works': 12, \"don't\": 12, 'get': 12, 'workgroups': 12, 'pc': 11, 'its': 11, 'include': 11, 'your': 11, 'by': 11, 'applications': 11, 'comes': 11, 'mouse': 11, 'many': 11, 'start': 11, 'problem': 10, 'based': 10, \"doesn't\": 10, 'zip': 10, 'format': 10, 'also': 10, 'me': 10, 'good': 10, 'logo': 10, 'better': 10, 'files': 10, 'true': 10, 'much': 10, 'runs': 9, 'buggy': 9}\n"
     ]
    }
   ],
   "source": [
    "print(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_context_dic = { word1: contexts_of(word1, texts2) for word1 in new_words }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40497088 -0.4164376   0.48043264 -0.39178229 -0.3831789   0.36858604\n",
      "  0.36149348  0.37907459  0.39572365 -0.42370758]\n"
     ]
    }
   ],
   "source": [
    "def get_avg_embed(word0, embed_mat1, word_dic1, word_context_dic):\n",
    "    context_count1 = word_context_dic[word0]        \n",
    "    words = word_dic1.words\n",
    "    total_count_sum = 0\n",
    "    embed = np.zeros(EMBEDDING_DIM)\n",
    "    for word1, count1 in context_count1.items():\n",
    "        if word1 in words:\n",
    "            ix1 = word_dic1.word2ix[word1]\n",
    "            embed += embed_mat1[ix1] * count1\n",
    "            total_count_sum += count1\n",
    "    if total_count_sum < 1:\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "    return embed/total_count_sum        \n",
    "        \n",
    "# need to skip oov words\n",
    "#!!!!!!!!!!!!!!!!11111\n",
    "### what if count is smaller than 10?\n",
    "\n",
    "temp = get_avg_embed('windows', mat1, word_dic1, new_word_context_dic)\n",
    "print(temp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2 = []\n",
    "for ix1 in range(n_words1, n_words2):\n",
    "    word2 = word_dic2.ix2word[ix1]\n",
    "    embed2 = get_avg_embed(word2, mat1, word_dic1, new_word_context_dic)\n",
    "    mat2.append(embed2)\n",
    "mat2 = np.array(mat2)\n",
    "mat2b = np.concatenate((mat1, mat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9583, 100)\n",
      "(5072, 100)\n",
      "(14655, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape)\n",
    "print(mat2.shape)\n",
    "print(mat2b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00670594 -0.01757532  0.00031279  0.02134672 -0.02010387  0.04631103\n",
      "  0.04829497  0.02501355  0.04532603  0.03562606 -0.00941096  0.04452807\n",
      " -0.00683808  0.02831235 -0.02694577  0.04215329 -0.02857791  0.01319558\n",
      "  0.03325592  0.02229548 -0.00657592  0.00780135 -0.01975346  0.00806315\n",
      " -0.04731954 -0.03876078 -0.00159435  0.01121148 -0.04123883  0.03608469\n",
      "  0.03906335 -0.03284369 -0.00498239 -0.02178704 -0.01848469  0.03944755\n",
      " -0.03038275 -0.00640755 -0.01900774  0.01959025 -0.01430454  0.04742635\n",
      " -0.00796503 -0.04415531 -0.00662602 -0.00692859  0.04312536  0.04147445\n",
      "  0.00102197 -0.02642416  0.02213148 -0.01989819  0.02945893  0.03492831\n",
      " -0.02178444 -0.00913808 -0.00861005  0.03066489 -0.04676073  0.04719165\n",
      " -0.03696543  0.03976751  0.03821139 -0.00107444  0.01274296  0.00871147\n",
      " -0.01314527  0.00583049  0.02715626  0.01377214  0.03178188  0.01826954\n",
      " -0.04220107  0.0061397   0.03155948  0.02434016  0.03245208 -0.04002186\n",
      " -0.00912543  0.02826928 -0.03327229  0.03838834 -0.03549304 -0.01916814\n",
      " -0.02903125 -0.00731605  0.01708927  0.02324262  0.02894298 -0.00366473\n",
      " -0.02065672  0.02892733 -0.03501866  0.02223912 -0.02796065 -0.02998239\n",
      "  0.01285103 -0.04217666  0.02222273  0.02990745]\n",
      "[-0.40497088 -0.4164376   0.48043263 -0.39178228 -0.3831789   0.36858603\n",
      "  0.36149347  0.3790746   0.39572364 -0.42370757 -0.26295292  0.43967804\n",
      "  0.45355177  0.30415237  0.45010567  0.49209198  0.3791281   0.47207144\n",
      "  0.39953962  0.36409777 -0.33171722  0.5392035   0.40686098  0.48024535\n",
      " -0.43163955 -0.34699208  0.36798593 -0.37360743  0.5266774  -0.28637207\n",
      "  0.41189626  0.44010797 -0.25990656 -0.45334312  0.48231655  0.34427533\n",
      " -0.36242384 -0.37478328  0.37052193 -0.38020515  0.40006706  0.449827\n",
      "  0.4255615   0.3676146  -0.3940083   0.4295669   0.41380915 -0.4328509\n",
      "  0.3661923  -0.3127236   0.3805727  -0.41361943 -0.40884745 -0.37693816\n",
      "  0.37011373  0.34728208 -0.3591048   0.4716149  -0.38806373  0.5027114\n",
      " -0.2858715   0.4755087   0.41551062 -0.40119678  0.38583776 -0.37922457\n",
      " -0.39326942 -0.3162254  -0.24624588  0.4035632   0.30842778 -0.41315407\n",
      "  0.3553757  -0.40036932  0.4554012   0.42330056  0.44687676  0.39337322\n",
      "  0.3134129   0.33528575 -0.43328932 -0.36996856  0.40188414 -0.37476495\n",
      " -0.33978158  0.4343802  -0.43611503 -0.43212014  0.46486923 -0.43281528\n",
      " -0.44449285 -0.50638086  0.38152492 -0.33694997  0.3687478   0.5132328\n",
      " -0.32863456  0.38801104  0.4795049  -0.3852616 ]\n",
      "Epoch 1/10\n",
      "350568/350568 [==============================] - 27s 76us/step - loss: 0.9974 - out1_loss: 0.2199 - lambda_14_loss: 0.7775\n",
      "Epoch 2/10\n",
      "350568/350568 [==============================] - 27s 76us/step - loss: 0.2805 - out1_loss: 0.0063 - lambda_14_loss: 0.2742\n",
      "Epoch 3/10\n",
      "350568/350568 [==============================] - 27s 77us/step - loss: 0.2471 - out1_loss: 2.1974e-04 - lambda_14_loss: 0.2469\n",
      "Epoch 4/10\n",
      "350568/350568 [==============================] - 27s 78us/step - loss: 0.2344 - out1_loss: 6.9320e-06 - lambda_14_loss: 0.2344\n",
      "Epoch 5/10\n",
      "350568/350568 [==============================] - 27s 78us/step - loss: 0.2129 - out1_loss: 3.0255e-07 - lambda_14_loss: 0.2129\n",
      "Epoch 6/10\n",
      "350568/350568 [==============================] - 28s 79us/step - loss: 0.1918 - out1_loss: 1.1921e-07 - lambda_14_loss: 0.1918\n",
      "Epoch 7/10\n",
      "350568/350568 [==============================] - 28s 79us/step - loss: 0.1771 - out1_loss: 1.1921e-07 - lambda_14_loss: 0.1771\n",
      "Epoch 8/10\n",
      "350568/350568 [==============================] - 28s 80us/step - loss: 0.1674 - out1_loss: 1.1921e-07 - lambda_14_loss: 0.1674\n",
      "Epoch 9/10\n",
      "350568/350568 [==============================] - 28s 80us/step - loss: 0.1604 - out1_loss: 1.1921e-07 - lambda_14_loss: 0.1604\n",
      "Epoch 10/10\n",
      "350568/350568 [==============================] - 28s 80us/step - loss: 0.1549 - out1_loss: 1.1921e-07 - lambda_14_loss: 0.1549\n",
      "[-0.737913    1.6844273   1.1184523  -0.50136465 -0.47675854 -4.4086647\n",
      " -4.4674644  -1.3325002  -3.3115702  -0.7049958   0.66091317  0.6800436\n",
      "  0.4631974  -5.2280664  -4.79684     1.5753874   0.63658315  1.2030878\n",
      "  0.50581783  0.4790921   1.0036182  -4.5055966   1.4497118   0.9257954\n",
      "  4.3348026  -0.6249227   0.47327325 -0.7655009  -4.417069    0.8345364\n",
      "  0.50019616  0.65136975  1.2623453  -0.6336216   1.1297662  -2.8060696\n",
      " -0.21961188 -0.35843572  0.3219897  -0.29589993  0.35125127  0.4633545\n",
      "  2.2505589   0.11793268 -0.6574106   0.8057911   2.4596374  -0.8481292\n",
      "  0.20118296 -1.7189938  -3.7126622  -0.41594124 -0.71244025 -0.37808287\n",
      " -4.7453904  -2.977078   -0.45125803  0.7423205  -0.50117916  1.4038678\n",
      "  5.4584174   0.8731496   0.9843548  -1.3902856   0.36877692 -0.5478621\n",
      "  1.673507    0.44280124  2.8140292   1.3085215  -3.3406687  -0.629082\n",
      " -2.087023   -1.0179532   0.85679704  0.65858626  2.4250824  -2.1433997\n",
      " -2.2804782   0.15183347 -0.77103597  5.0000443   0.6251046  -1.6330228\n",
      "  3.470114    0.665963   -0.8156534   3.774197    0.7500365  -0.7747642\n",
      " -1.0221838   4.38383     0.35567778 -0.06848744 -4.930268   -3.762273\n",
      "  4.283878   -1.8996623   2.131977   -0.23273602]\n"
     ]
    }
   ],
   "source": [
    "cbow2 = Cbow(n_window=N_WINDOW, n_negative=N_NEGATIVE, embed_dim=EMBEDDING_DIM, word_dic=word_dic2)\n",
    "input2, target2 = cbow2.get_train_data(texts2)\n",
    "model2 = cbow2.get_network()\n",
    "\n",
    "print(cbow2.get_embed('windows'))\n",
    "cbow2.set_embed(mat2b)\n",
    "model2 = cbow2.get_network()\n",
    "print(cbow2.get_embed('windows'))\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "score2 = model2.fit(x=input2, y=target2, batch_size=100, epochs= 10)\n",
    "#score2 = model2.fit(x=[cbow2.data_x, cbow2.data_context, cbow2.data_negative], y=[cbow2.target_data, cbow2.target_negative], batch_size=100, epochs= 1)\n",
    "print(cbow2.get_embed('windows'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow1type(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
