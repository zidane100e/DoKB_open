{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN을 이용한 감성분석\n",
    "영화평으로 부터 평점을 예측하는 모델이 잘 알려져 있습니다.  \n",
    "\n",
    "1. 데이터 : IMDB movie-review classification problem\n",
    "    * 긍정, 부정으로만 분리\n",
    "2. 사용 모델 : RNN, LSTM  \n",
    "\n",
    "module import 에서 세부적인 함수, 클래스를 직접 호출하면 사용 시 전체 모듈명의 생략이 가능합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bwlee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어를 정형화 하기 위해 index 로 표시합니다.  \n",
    "사용하고자 하는 단어의 수를 정하고, 이 단어 수를 벗어날 만큼 사용 빈도가 낮은 단어는 무시하는 것이 일반적입니다.  \n",
    "아래의 변수 num_words 가 이 역할을 합니다.  \n",
    "문장의 길이가 각 instance 마다 다르므로 전체 단어 갯수에 대한 큰 값을 하나 정하고 그 이상 넘어가는 문장의 단어는 무시합니다.  \n",
    "sequence.pad_sequences 함수가 maxlen 만큼의 단어들을 골라냅니다. 이 때 앞쪽을 무시할 지, 뒷쪽을 무시할 지는 사용자가 입력으로 넣어주게 됩니다.  \n",
    "Default 로는 앞쪽을 무시합니다. 아래의 결과에서 확인 가능합니다.  \n",
    "단어수가 그 이하일 경우 빈 공간은 0 혹은 사용자 지정값으로 채워 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 9s 5us/step\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "(25000, 80) (25000, 80)\n",
      "[   15   256     4     2     7  3766     5   723    36    71    43   530\n",
      "   476    26   400   317    46     7     4 12118  1029    13   104    88\n",
      "     4   381    15   297    98    32  2071    56    26   141     6   194\n",
      "  7486    18     4   226    22    21   134   476    26   480     5   144\n",
      "    30  5535    18    51    36    28   224    92    25   104     4   226\n",
      "    65    16    38  1334    88    12    16   283     5    16  4472   113\n",
      "   103    32    15    16  5345    19   178    32]\n"
     ]
    }
   ],
   "source": [
    "num_words = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_train[0])\n",
    "word2ix = imdb.get_word_index()\n",
    "word2ix = {word: (ix+3) for word, ix in word2ix.items()}\n",
    "word2ix[\"<PAD>\"] = 0\n",
    "word2ix[\"<START>\"] = 1\n",
    "word2ix[\"<UNK>\"] = 2\n",
    "ix2word = {i : word for word, i in word2ix.items()}\n",
    "text = ' '.join([ix2word[i] for i in x_train[0]])\n",
    "print(text)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "index 로 표현된 단어들은 데이터에 서로 간 연관성이 나타나지 않습니다.  \n",
    "단어들을 적은 차원의 데이터 값들로 표현할 수 있습니다. 이 때 연관된 단어들은 가까운 위치로 mapping 하게 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,593,025\n",
      "Trainable params: 2,593,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def rnn_network():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 128))\n",
    "    model.add(SimpleRNN(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "rnn = rnn_network()\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 13s 503us/step - loss: 0.5951 - acc: 0.6653\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 12s 472us/step - loss: 0.4434 - acc: 0.8026\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 12s 466us/step - loss: 0.3377 - acc: 0.8608\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 12s 467us/step - loss: 0.2415 - acc: 0.9064\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 12s 467us/step - loss: 0.1892 - acc: 0.9290\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 12s 468us/step - loss: 0.2754 - acc: 0.8859\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 12s 468us/step - loss: 0.1435 - acc: 0.9480\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 12s 470us/step - loss: 0.0876 - acc: 0.9705\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 12s 469us/step - loss: 0.0822 - acc: 0.9717\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 12s 470us/step - loss: 0.0477 - acc: 0.9856\n"
     ]
    }
   ],
   "source": [
    "history = rnn.fit(x_train, y_train, batch_size=batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def lstm_network():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "rnn2 = lstm_network()\n",
    "rnn2.summary()\n",
    "rnn2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 0.4605 - acc: 0.7861\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 0.3050 - acc: 0.8744\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 0.2205 - acc: 0.9148\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.1530 - acc: 0.9433\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.1136 - acc: 0.9587\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.0839 - acc: 0.9699\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.0612 - acc: 0.9796\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.0489 - acc: 0.9844\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.0389 - acc: 0.9868\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.0252 - acc: 0.9921\n"
     ]
    }
   ],
   "source": [
    "history2 = rnn2.fit(x_train, y_train, batch_size=batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
