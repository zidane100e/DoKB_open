{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras review and NLP Intro.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### 지주 데이터전략부 이병욱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of contents\n",
    "1. [Keras review](#keras_review)\n",
    "    1. [Keras intro](#keras_intro)\n",
    "    2. [Keras advanced_skills](#keras_advanced_skills)\n",
    "2. [NLP intro](#nlp_intro)\n",
    "    1. [NLP application](#nlp_application)\n",
    "    2. [한글 처리](#한글_처리)\n",
    "3. [Understanding text](#understanding_text)\n",
    "    1. [One hot encoding](#one_hot_encoding)\n",
    "    2. [Word embedding](#word_embedding)\n",
    "        1. [Word2vec](#word2vec)\n",
    "        2. [Embedding_layer](#embedding_layer)\n",
    "        3. [DNN for text analysis](#dnn_for_text)\n",
    "    3. [Sentence embedding](#sentence_embedding)\n",
    "        1. [Basic models](#baic_models)\n",
    "        2. [Advanced models](#advanced_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras review <a name=\"keras_review\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras intro <a name=\"keras_intro\"></a>\n",
    "* Keras : High level API of tensorflow, 초기 습득 간편\n",
    "    * https://keras.io/\n",
    "    * https://blog.keras.io/index.html\n",
    "    * https://github.com/keras-team/keras\n",
    "\n",
    "* [Deep learning process in Keras 예제](Keras_intro/keras_in_30sec.ipynb)\n",
    "    1. Data preparation\n",
    "    2. Model construction\n",
    "    3. Model training\n",
    "    4. Model evaluation\n",
    "\n",
    "* [MLP 예제](Keras_intro/mnist_mlp.ipynb)\n",
    "    * mnist dataset 에 대하여 전체 point 에 대한 MLP 적용\n",
    "* [CNN 예제](Keras_intro/mnist_cnn.ipynb)\n",
    "    * mnist dataset 에 대하여 2D 그림에 대하여 CNN(2D filter) 적용\n",
    "* [RNN 예제](Keras_intro/imdb_lstm.ipynb)\n",
    "    * IMDB 무비 리뷰에 대하여 긍부정 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras advanced skills <a name=\"keras_advanced_skills\"></a>\n",
    "* [Callbacks 예제](Keras_intro/ex_callbacks.ipynb)\n",
    "* [User custom operation: Lambda 예제](Keras_intro/ex_lambda.ipynb)\n",
    "* User custom layer\n",
    "    * 모든 레이어들이 정의되는 기본적인 형식\n",
    "        * Layer1 = Specific1(Interface)\n",
    "        * Layer2 = Specific2(Interface)\n",
    "    * [custom definition](https://keras.io/layers/writing-your-own-keras-layers/)  \n",
    "    세 가지 함수의 직접 구현 필요        \n",
    "        * build(input_shape) : weight 등의 구성\n",
    "        * call(x) : 계산 수행\n",
    "        * compute_output_shape(input_shape) : 결과 서술\n",
    "    * [Example, Dense layer : line 796](https://github.com/keras-team/keras/blob/master/keras/layers/core.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Intro <a name=\"nlp_intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP application  <a name=\"nlp_application\"></a>\n",
    "* 검색(단순 비교의 확장)\n",
    "    * 유사 사용어 : 국민 --> (국민은행, 신한은행, KEB, 은행, 국민카드, ...)\n",
    "    * 유사 철자 : 국민 --> (국만, 굼긴, 궁민, ...)\n",
    "    * 문서 내 특정 구문 추출\n",
    "* text classification : 주제 분류, 감성 분석 (성능 word count < word embedding < sentence embedding)\n",
    "* text similarity, 의미 관련성 : FAQ, QA\n",
    "* 오류 발견 등의 언어 처리 \n",
    "    \n",
    "### Practice   \n",
    "* 웹, 모바일, 챗봇 등에서 명령어 이해 (X 클릭 --> 클릭 --> 클릭)\n",
    "* 뉴스 기사 검색, 분류 등 (특정 기업, 경기, ...) \n",
    "* 페이지 내 필요 정보/상품 검색(query --> document)\n",
    "* 로그 분석\n",
    "    * ID 기반 분석 : page1 --> page2 --> page3 (page간 관련성 고려되지 않음)\n",
    "    * 페이지 내용 기반 분석 : page1(content1, content2) --> page2(content2, content3) --> page3(content3, ...)\n",
    "* 자동화 : 약관 내 특정 구문 발견 및 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 한글 처리 <a name=\"한글_처리\"></a>\n",
    "* Text 이해의 첫 단계 : Tokenizer\n",
    "* 영어 : (simplest case) 공백에 따른 분할\n",
    "* 한글 : 단어에 대한 변형이 심해 형태소분석 필요\n",
    "    * 오픈소스 형태소 분석기 : Komoran, KKMA, Mecab-KO, SoyNLP, ...\n",
    "        * [한글 처리 파이썬 라이브러리 : konlpy](https://konlpy-ko.readthedocs.io/ko/v0.5.1/) 다수 형태소 분석기 포함\n",
    "    * 형태소 분석기 비교\n",
    "        * [비교링크1](https://iostream.tistory.com/144?utm_source=gaerae.com&utm_campaign=%EA%B0%9C%EB%B0%9C%EC%9E%90%EC%8A%A4%EB%9F%BD%EB%8B%A4&utm_medium=social&fbclid=IwAR2Rl5nkJza7eAPf2nelK67JqhS07OIjMuS9G77jb3JqTuNSQ_JwwXL30eg)\n",
    "        * [비교링크2](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/05/10/postag/)        \n",
    "        * Komoran은 세종말뭉치와 동일한 품사태그 적용\n",
    "    * 추가 고려 사항 (자주 반복되는 복합 단어 생성)\n",
    "        * https://github.com/lovit/sejong_corpus_cleaner\n",
    "        * 세종말뭉치는 국립국어원에서 배포한 한글 분석을 위한 기본 자료\n",
    "        * 말뭉치 내 형태소분석 자료가 있으나 배포 형식이 바로 사용에 어려움\n",
    "        * 위의 사이트에서 세종말뭉치 내 자료 추출을 전처리 util 제공\n",
    "\n",
    "            \n",
    "#### 한글 전처리 적용 예정 방법(추천) : Komoran + (LR composition in soynlp)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding text <a name=\"understanding_text\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest representation : One hot encoding <a name=\"one_hot_encoding\"></a>\n",
    "### 텍스트를 어떻게 표현하는가?\n",
    "* ? (예시) 콜센터 기록에 대하여 주제 분류, 감성분석, 유사 기록 탐색 등을 해보고 싶다면?\n",
    "* 처리 방법\n",
    "    1. 문장을 보고 긍정적인 단어가 포함되어 있는지 확인 (단어 확인)\n",
    "    2. 신경망에 단어를 넣고, 주제 분류문제로 해결\n",
    "* 두 방법 구현에서 고려할 점\n",
    "    1. 단어가 많아지면 비교에 일이 많아짐(비교는 어떻게?)\n",
    "    2. 신경망에 단어를 어떻게 넣을 수 있을까?\n",
    "* 가장 간단한 형태의 표현법 : One hot encoding(~Interfacing)    \n",
    "\n",
    "* 단어 비교 예시  \n",
    "    * 확인할 문장 : \"삼성전자의 영업이익이 30% 증가하였다\"  \n",
    "    * 찾을 단어 : \"영업이익\", \"증가\"  \n",
    "    * (간단히 생각하기 위해) 찾을 단어 : \"영업이익이\", \"증가하였다\"  \n",
    "    * 프로그래밍에서 각 단어를 string 으로 유지? --> 숫자에 비해 비효율적\n",
    "        * 각 단어별로 숫자 id 할당 --> \n",
    "[One hot encoding Ex.](examples/id_encode.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One hot encoding (계속)\n",
    "* ID는 여전히 신경망 변수로 쓸 수 없음 : id: 2는 id: 1의 두 배가 아님\n",
    "* 개별적인 변수로 취급 필요\n",
    "    > [0, 1, 2, 3] -->   \n",
    "    > [[0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0]]\n",
    "    \n",
    "    > [참고] Bit type   \n",
    "    * 실제 계산에서 One hot encoding 과 같은 id를 사용할 때는 int 타입보다는 bit type 효율적\n",
    "    * [0, 0, 0, 1] : 0, 1 은 integer type --> 4\\*8 byte = 4\\*8\\*8 bit  \n",
    "    * 실제 bit 로 표현하고 싶은 경우 bit 단위 data type 필요  \n",
    "    * C, python 에서는 ctype 이용 --> class 정의 또는 다른 오픈소스 라이브러리 활용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embedding <a name=\"word_embedding\"></a>\n",
    "* One hot encoding 의 문제점\n",
    "    1. id 간 연결성이 없음\n",
    "    2. 한 개 값만 1이고 나머지 값은 모두 0으로 빈공간이 많아 비효율적 \n",
    "    3. vocabulary 사이즈에 따라 표현을 위한 크기 비례\n",
    "        * voc 크기는 수십만 이상\n",
    "        * 주요 사용 단어만 선택해도 수만 이상\n",
    "        * 신경망에 사용하게 되면 더 증가 (layer 연결을 위한 weights는 n_node_layer1 * n_node_layer2)\n",
    "* --> word embedding 사용   \n",
    "\n",
    "### word embedding\n",
    "단어가 쓰일 때 주변 단어들과 연관성 있는 것을 고려하여 단어를 표현\n",
    "* 단어 간 연관성 예시\n",
    "    * 내가 간다, 내가 많이 먹었다, 내가 바다 한다(X)\n",
    "* 글자 간 연관성 예시\n",
    "    * 국어, 국가, 국민, 국취(X)\n",
    "    * ㄱ ㅜ, ㄱ ㅗ, ㄱ ㅏ, ㄱ ㄱ(X)\n",
    "* 바라보고자 하는 입력변수 간 상호 연관성 반영하여 수십만 개 이상의 단어를 ~수백 개 차원 내 값으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embedding (계속)\n",
    "* word embedding 주요 방법들\n",
    "    * Word2vec(word context)\n",
    "        * https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    * Glove(Word co-ourrence)\n",
    "        * https://nlp.stanford.edu/projects/glove/\n",
    "        * https://github.com/stanfordnlp/GloVe\n",
    "    * fasttext(Word + subword)\n",
    "        * https://fasttext.cc/\n",
    "        * https://radimrehurek.com/gensim/models/fasttext.html\n",
    "    * Wordpiece (subword occurrence) : Multi-language 등을 하기에 적합한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embedding __ Word2vec <a name=\"word2vec\"></a>\n",
    "* LM(Language modeling) : 현재까지의 단어로 다음 단어를 추정\n",
    "* Word2vec : LM 을 간단하게 구현, 주변 단어로 타겟단어 예측  \n",
    "\"이번에 발표된 삼성전자의 영업이익이 지난해 보다 30% 증가하였다\" 가 있는 경우\n",
    "    * CBOW\n",
    "        * ((\"이번에\", \"발표된\", \"영업이익이\", \"지난해\"), \"삼성전자의\"),  \n",
    "        * ((\"발표된\", \"삼성전자의\", \"지난해\", \"보다\"), \"영업이익이\"), ...  \n",
    "        * 입력 context 에 대하여 평균 취함(위에서 4개 단어 1개 단어로)  \n",
    "    * SKIP gram\n",
    "        * 의미는 동일하지만 평균을 취하지 않고, 주변 각 단어와 타겟 단어를 데이터 쌍으로 묶음\n",
    "        * (\"삼성전자의\", \"이번에\"), (\"삼성전자의\", \"발표된\"), ...  \n",
    "        * (\"영업이익이\", \"발표된\"), (\"영업이익이\", \"삼성전자의\"), ...\n",
    "    * CBOW vs skip-gram\n",
    "        * CBOW 가 수렴이 빠른 반면 데이터 커질 수록 skip-gram 이 성능은 조금 나은 것으로 알려짐\n",
    "        * 데이터 크면 skip-gram, 작으면 CBOW 사용 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embedding __ negative sampling (skip)\n",
    "* 특징 기술\n",
    "    * sub-sampling  \n",
    "        * 단어에 따른 사용 빈도는 매우 큰 차이 나타냄    \n",
    "          (~ 대부분의(90%?) 문장에서 쓰이는 단어는 몇 천개 수준)  \n",
    "    * negative sampling  \n",
    "        * 단어의 예측의 경우 신경망의 최종 layer로 softmax가 일반적\n",
    "        * 최종 분류 값에 대한 각 값의 확률 값을 나타내므로,  \n",
    "        전체 단어가 나타날 확률을 구한 후 최고 값을 가지는 단어를 타겟으로 선택  \n",
    "        \n",
    "<img src=\"pic/negative_sampling.jpg\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embedding __ Out of vocabulary\n",
    "#### OOV\n",
    "    * word embedding 에서 사용하는 vocabulary 구성 단어의 갯수 정해져야함  \n",
    "    * 단어가 많을수록 구현에 어려움 있으며, 단어 사용 빈도 등으로 제한  \n",
    "    * 전문 용어 등은 빈도는 낮아도 특별히 포함시킬 필요 있음 (개별적 입력 필요)\n",
    "    \n",
    "#### 사전 확장 예시\n",
    "1. 계열사 데이터로 작은 사전 구축(모델 구현 쉽고, 데이터 수집 쉬움)  \n",
    "2. 지주에서 큰 사전 구축    \n",
    "2. 작은 사전 단어와 큰 사전 단어 간 임베딩 분포의 선형적 변환 행렬 T 구함  \n",
    "3. 계열사에서 새 데이터가 들어오는 경우 큰 사전에 대하여 T를 적용하여 작은 사전에서의 embedding 값 알 수 있음   \n",
    "\n",
    "#### 단어 추가 예시\n",
    "1. 큰 사전에 없는 전문 용어 발생  \n",
    "2. 전문용어 포함된 작은 데이터셋으로 word embedding 학습    \n",
    "2. 작은 사전 단어와 큰 사전 단어 간 임베딩 분포의 선형적 변환 행렬 구함  \n",
    "3. 전문용어에 변환행렬 적용   \n",
    "\n",
    "<img src=\"pic/oov_update.jpg\" width=\"500\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embedding \\_\\_ Embedding layer<a name=\"embedding_layer\"></a>\n",
    "* https://keras.io/layers/embeddings/\n",
    "\n",
    "```\n",
    "class Embedding(Layer)  # 임베딩도 layer 의 한부분\n",
    "\n",
    "from keras.layers import Embedding    \n",
    "embed = Embedding(n_input, n_dim)  \n",
    "```\n",
    "\n",
    "* Word Embedding 적용을 위한 방법\n",
    "    1. specific embedding : 특정 모델에 적합한 단어 임베딩을 포함해 학습\n",
    "        * 성능 우수\n",
    "    2. pretrained embedding : 외부에서 만든 단어 임베딩을 이용해 특정 과제를 위한 분류기만 구성\n",
    "        * 라벨 적을 경우 우수\n",
    "    <img src=\"pic/embedding.jpg\" width=\"500\"/>  \n",
    "* [pretrained word model 사용 개념](examples/pretrained_word.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embedding \\_\\_ DNN to understand text<a name=\"dnn_for_text\"></a>\n",
    "* sentiment analysis\n",
    "    * RNN model : [imdb 감성분석 예제](examples/imdb_sentiment.ipynb)  \n",
    "    * CNN model : [Conv 사용 예제 링크](examples/6.4-sequence-processing-with-convnets.ipynb)  \n",
    "\n",
    "* RNN model\n",
    "    * wait --> for --> the --> ... --> rent --> it --> (긍정, 부정)\n",
    "    \n",
    "* CNN model\n",
    "    * [참고 Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181)      \n",
    "    <img src=\"pic/CNN_sentiment.PNG\" width=\"600\"/>  \n",
    "    Embedding 에 CNN 적용 --> 단어들이 가지는 특징 추출 --> 분류기 추가 구성  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Embedding <a name=\"sentence_embedding\"></a>\n",
    "* 단어 보다는 문장, 문단에 대한 이해의 필요성 더 큼\n",
    "    * sentence similarity, text classification, ...\n",
    "* 문장, 문단에 대한 embedding이 가능하면 기존 DNN 의 많은 부분에 대한 고려 필요 없어짐  \n",
    "    * --> 활용성, 편의성 증대\n",
    "* sentence similarity는 직접적으로 수행 가능, 문단 내 sentence 비교 등을 통해 문단으로 확대 가능\n",
    "\n",
    "### sentence = f(words)\n",
    "* Basic models : word embedding에 대해 직접적으로 관계식 f(words) 부여\n",
    "    * sentence.count() # 긍부정 단어 개수 얻기\n",
    "    * sentence.average() # average embeddings of words\n",
    "    * sentence.average().modify() # 평균 분포 등을 변형\n",
    "* Advanced models : word 간 관계식을 Language Model 등으로부터 학습을 통하여 f(words) 찾기\n",
    "    * Skip thought : 문장과 주변 문장간 의미들의 연관성을 고려하여 임베딩\n",
    "        * Skip gram + seq2seq\n",
    "        * Big size (~300 dim --> ~4000 dim)\n",
    "        * LM : 주어진 단어들로부터 다음 단어 확인\n",
    "        * 다수 모델의 기본 아이디어\n",
    "    \n",
    "seq2seq 구조  \n",
    "<img src=\"pic/seq2seq.png\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic models \\_\\_ DAN(Deep averaging network) <a name=\"basic_models\"></a>\n",
    "* Implication : 문장을 단어들의 평균값으로 취한 후 딥러닝을 적용하여도 성능이 나쁘지 않음\n",
    "    * [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](http://www.aclweb.org/anthology/P15-1162)  \n",
    "<img src=\"pic/dan.jpg\" width=\"600\"/>  \n",
    "* 위의 데이터셋은 모두 감성분석 자료, SST fine 은 5개로 상세 분류  \n",
    "* 문장을 단순히 단어들의 평균으로 만든 모델도 의미 있는 결과 나타냄(NBOW)  \n",
    "    * 워드 임베딩 값에 평균을 취한 후 softmax 로 분류  \n",
    "* 평균값에 MLP 적용한 DAN은 NBOW 대비 성능 향상 및 구조 있는 모델과 비교할 정도 수준  \n",
    "    * NBOW(워드 임베딩 평균), RecNN(recursive NN), CNN-MC(CNN multi channel), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic models \\_\\_ Power mean\n",
    "* Implication : 여러 임베딩을 결합하고, 특징을 부각시킬 때 성능 향상\n",
    "    * [Concatenated Power MeanWord Embeddings as Universal Cross-Lingual Sentence Representations](https://arxiv.org/abs/1803.01400)\n",
    "    * https://github.com/UKPLab/arxiv2018-xling-sentence-embeddings  \n",
    "* power mean  \n",
    "<img src=\"pic/p_mean1.jpg\" width=\"350\"/>  \n",
    "* 평균값을 사용할 경우 개별 word 의 특성이 희석\n",
    "* power값에 따라 특정한 특성이 강조 또는 감소 --> 여러 power 값을 결합하면 특징 포착에 효과적\n",
    "* power mean = {4개 종류의 특성이 상이한 embedding 결합}*(1: avg, -inf: min avg, inf: max avg)\n",
    "\n",
    "<img src=\"pic/p_mean2.jpg\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence embedding __ Power mean (계속)\n",
    "* 평균 값에 logistic regression(softmax) 을 통한 단순 분류기 적용\n",
    "* 여러 종류의 sentence embedding 와 비슷한 수준의 성능  \n",
    "<img src=\"pic/p_mean3.jpg\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ Concept <a name=\"advanced_models\"></a>\n",
    "신경망 중 문장을 표현하는 구조가 포함된 경우 해당 구조를 sentence embedding 으로 활용  \n",
    "\n",
    "ex1. [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107?context=cs.LG)  \n",
    "문장의 의미를 이해한 후 다른 언어로 번역  \n",
    "<img src=\"pic/encoder1.png\" width=\"500\"/>  \n",
    "ex2. [AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS](https://arxiv.org/abs/1803.02893)  \n",
    "문장의 의미를 이해해야 하는 supervised learning  \n",
    "<img src=\"pic/encoder2.PNG\" width=\"500\"/>  \n",
    "\\[참고\\] [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ Concept (계속)\n",
    "sentence embedding으로 활용 가능한 주요 방법들   \n",
    "* Feature based approaches : ELMO, Universal sentence encoder, ...  \n",
    "    * pre-trained 모델로부터 얻은 값들을 기존 모델에 feature로 사용(Transfer model)\n",
    "    * 사용하는 과제에 따라 추가 적용하는 모델 형태 다름\n",
    "* Fine-tuning based approaches : GPT, BERT\n",
    "    * pre-trained 모델에 대해 손쉽게 Fine-tuning\n",
    "        * 입력 정보의 의미 추출하는 부분이 설계 구조에 포함되어 기존 모델 추가 개발 필요 없음\n",
    "    * 현재 가장 높은 성능 나타내나 필요 리소스 큼\n",
    "    * 개발 모델을 공개하여 (영어의 경우) NLP 플랫폼으로 사용 가능\n",
    "    \n",
    "> Contextualized word embedding(ELMO)?  \n",
    "    * 쓰이는 문장에 따라 각 단어의 embedding 값이 달라짐  \n",
    "        * word2vec 등은 한 개의 단어는 한 개의 값만 표현  \n",
    "    * Neural language model, Neural machine translator model의 중간 layer를 feature로 사용   \n",
    "        * 주변 단어들에 영향을 받아 변하는 dynamic embedding  \n",
    "        * 단어가 사용되는 구문에서의 의미 반영 가능\n",
    "        \n",
    "> 사용 데이터 규모 예시 : BookCorpus dataset  \n",
    "    * 책수 : ~ 1만, 문장수 : ~ 7천만, 총단어수 : ~10억, unique words : 1백3십만, 문장 평균 단어 수 : 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [참고] Transfer learning\n",
    "<img src=\"pic/transfer_cnn.jpg\" width=\"600\"/>  \n",
    "* 공통적인 부분은 해당 데이터가 아닌 일반적인 데이터의 사용 가능\n",
    "    * 영상 분석 적용 : 다수의 공개 데이터로 학습 후 필요한 도메인 데이터로 추가 학습   \n",
    "    * NLP 적용 예시 : 문장 분류 모델 구성 시 sentence embedding을 얻어오고 분류기만 추가 개발\n",
    "* 데이터 부족한 과제에서 효율적\n",
    "* [참고] https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ Universal sentence encoder\n",
    "기존 모델 대비 성능 향상 확인  \n",
    "* [Universal sentence encoder 예제](examples/use_test.ipynb)\n",
    "* USE_E : DAN(속도 우수), USE_T : Transformer(성능 우수)   \n",
    "<img src=\"pic/universal_encoder1.PNG\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ Universal sentence encoder (계속)\n",
    "데이터가 적을수록 transfer learning 효과 커짐  \n",
    "<img src=\"pic/universal_encoder2.jpg\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ ELMO\n",
    "데이터가 적은 경우에서 더욱 효과적  \n",
    "<img src=\"pic/elmo_res.PNG\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced models \\_\\_ ELMO (계속)\n",
    "동음이의어의 구별 예시  \n",
    "\n",
    "<img src=\"pic/elmo_result.png\" width=\"700\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KB에서의 NLP 적용을 위한 고려사항\n",
    "* 과제 진행에서 흔히 겪는 어려움  \n",
    "    * 제대로된 레이블이 많지 않음\n",
    "    * 개별적인 모델 구성을 위한 인력 많지 않음\n",
    "    * --> __Transfer learning__ 효과적\n",
    "* Transfer learning process\n",
    "    1. 빅데이터로 Pre-train  \n",
    "        * 다양한 주제의 데이터가 일반화에 유리\n",
    "    2. 해결하고자 하는 도메인 영역의 스몰데이터로 Fine-tune (optional)\n",
    "    3. Pre-train 으로 얻어지는 값들을 피쳐로 사용하는 특정 모델 구축\n",
    "\n",
    "> Pre-train의 효율적 구현방안  \n",
    ">* supervised learning을 사용하는 경우 의미에 대한 학습시 적은 데이터로도 효과 (SNLI dataset)  \n",
    ">* (sent1, sent2)가 주어졌을 때 유사한가, 그렇지 않은가에 대한 레이블  \n",
    ">* SNLI 데이터 (~50만)를 번역기 등을 이용하여 번역 후 활용하는 것도 고려할 수 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KB에서의 NLP 적용을 위한 고려사항 (계속)\n",
    "### CoE에서의 구축 목표\n",
    "* pre-training을 통해 sentence embedding 생성 후 API 로 제공 계획\n",
    "    > sent1 = \"######\"  \n",
    "    > embed1 = get_embed(sent1)  \n",
    "* 참가 직원은 현업에서 pretrained model을 가지고 현업데이터로 finetune\n",
    "    * 내부 과제 진행 시 파일 복사등을 통해 내부 구축 가능\n",
    "    > model1 = load_model(pretrained_model)  \n",
    "    model1.train(new_data)  \n",
    "    model1.evaluate(new_data)  \n",
    "    \n",
    "#### 구축 계획\n",
    "자율참여 적극환영  \n",
    "* 한글 사전에 대한 word embedding 적용 \n",
    "    * Sejong corpus, book, news\n",
    "* sentence embedding\n",
    "    * baseline : power mean (Glove, word2vec, fasttext?, postprocessing)\n",
    "    * advanced : ELMO, universal sentence encoder, quick-thought, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ELMO (Embeddings from  language model)\n",
    "* Get ELMO vertor from pretrained model\n",
    "    * (optionally) Give ELMO vector into hidden and output layer\n",
    "* Give EMLO vector + original input into original task\n",
    "    * (optionally) Finetune the model\n",
    "        * Whole architecture\n",
    "        * Pretrained model\n",
    "        * similar in sentiment analysis\n",
    "* 사용 데이터 : 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
