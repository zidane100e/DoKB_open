{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## which noun extractor to use?\n",
    "* I compared noun extractor in soynlp which has two version, v1 and v2.\n",
    "* v2 compound version includes many ~한다\n",
    "* v2 non-compound also include 한다\n",
    "* In addition, v2 has much more noun.\n",
    "* So I decided to use v1 with some filtering like \"2글자 이상\", \"&영어\", '#%' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "from konlpy.tag import Komoran, Kkma\n",
    "import soynlp\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = pymysql.connect(host='localhost', port=3306,user='*****', passwd='*****',\n",
    "                        db='*****', charset='utf8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'select news_content from kb_kor_news_code_clean1;'\n",
    "df1 = pd.read_sql(query1, db1)\n",
    "contents = df1['news_content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420358"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whole news are too big to calculate.  \n",
    "Chunk each 50000 news and get nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = range(0, len(contents), 50000)\n",
    "\n",
    "contentss, steps = [], []\n",
    "for i_x, i_r in enumerate(edge[1:]):\n",
    "    i_l = edge[i_x]\n",
    "    steps.append([i_l, i_r])\n",
    "    \n",
    "if steps[-1][1] < len(contents):\n",
    "    steps[-1][1] = len(contents)\n",
    "for i_l, i_r in steps:\n",
    "    contentss.append( contents[i_l:i_r] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuss = [ DoublespaceLineCorpus(content1, iter_sent=True) for content1 in contentss ]\n",
    "#corpus = DoublespaceLineCorpus(contents, iter_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 686738 from 933263 sents. mem=1.239 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10172514, mem=2.962 Gb\n",
      "[Noun Extractor] batch prediction was completed for 226032 words\n",
      "[Noun Extractor] postprocessing detaching_features : 72746 -> 72440\n",
      "[Noun Extractor] postprocessing ignore_features : 72440 -> 72364\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72364 -> 72028\n",
      "[Noun Extractor] 72028 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=3.307 Gb                    \n",
      "[Noun Extractor] 79.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 663963 from 896662 sents. mem=3.480 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9794859, mem=4.114 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222206 words\n",
      "[Noun Extractor] postprocessing detaching_features : 70625 -> 70344\n",
      "[Noun Extractor] postprocessing ignore_features : 70344 -> 70267\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70267 -> 69992\n",
      "[Noun Extractor] 69992 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.059 Gb                    \n",
      "[Noun Extractor] 78.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 703419 from 937179 sents. mem=4.113 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10725169, mem=4.343 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240452 words\n",
      "[Noun Extractor] postprocessing detaching_features : 73967 -> 73660\n",
      "[Noun Extractor] postprocessing ignore_features : 73660 -> 73577\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73577 -> 73268\n",
      "[Noun Extractor] 73268 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.323 Gb                    \n",
      "[Noun Extractor] 79.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 688596 from 944891 sents. mem=4.323 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10597120, mem=4.392 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235483 words\n",
      "[Noun Extractor] postprocessing detaching_features : 71965 -> 71671\n",
      "[Noun Extractor] postprocessing ignore_features : 71671 -> 71594\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71594 -> 71286\n",
      "[Noun Extractor] 71286 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.376 Gb                    \n",
      "[Noun Extractor] 79.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 697154 from 922282 sents. mem=4.375 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10704493, mem=4.358 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225952 words\n",
      "[Noun Extractor] postprocessing detaching_features : 74353 -> 74053\n",
      "[Noun Extractor] postprocessing ignore_features : 74053 -> 73978\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73978 -> 73653\n",
      "[Noun Extractor] 73653 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.343 Gb                    \n",
      "[Noun Extractor] 79.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 688995 from 915339 sents. mem=4.338 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10787258, mem=4.343 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223664 words\n",
      "[Noun Extractor] postprocessing detaching_features : 72790 -> 72478\n",
      "[Noun Extractor] postprocessing ignore_features : 72478 -> 72403\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72403 -> 72095\n",
      "[Noun Extractor] 72095 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.340 Gb                    \n",
      "[Noun Extractor] 79.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679653 from 949810 sents. mem=4.340 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10720878, mem=4.341 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229289 words\n",
      "[Noun Extractor] postprocessing detaching_features : 72089 -> 71810\n",
      "[Noun Extractor] postprocessing ignore_features : 71810 -> 71736\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71736 -> 71417\n",
      "[Noun Extractor] 71417 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.346 Gb                    \n",
      "[Noun Extractor] 79.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 881448 from 1308544 sents. mem=4.365 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15272794, mem=4.813 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280774 words\n",
      "[Noun Extractor] postprocessing detaching_features : 91679 -> 91296\n",
      "[Noun Extractor] postprocessing ignore_features : 91296 -> 91207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91207 -> 90786\n",
      "[Noun Extractor] 90786 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=4.805 Gb                    \n",
      "[Noun Extractor] 80.35 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "# 가-힣 한글 범위\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=False)\n",
    "whole_dic = {}\n",
    "for corpus1 in corpuss:\n",
    "    nouns = noun_extractor.train_extract(corpus1)\n",
    "    for key1, val1 in nouns.items():\n",
    "        #if len(key1)>1 and val1.frequency > 10 and val1.score > 0.4 and re.search(\"(&[a-zA-Z1-9]+|\\d+\\.*%|\\d+.원|\\d+억|\\d+조)\", key1) is None:\n",
    "        if len(key1)>1 and val1.frequency > 10 and val1.score > 0.4 and re.search(\"[^가-힣]\", key1) is None:\n",
    "            whole_dic.setdefault(key1, 0)\n",
    "            whole_dic[key1] += val1.frequency * val1.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "f1_s = \"corpus/sejong_corpus_cleaner/scripts/word.pk\"\n",
    "with open(f1_s, 'rb') as f1:\n",
    "    dic1 = pk.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_keys = []\n",
    "for noun_tag in dic1.keys():\n",
    "    elms = noun_tag.split('\\t')\n",
    "    if len(elms) > 1:\n",
    "        dic_keys.append(elms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 메타헬스케어투자조합\n",
      "--- 0\n",
      "--- 5000\n",
      "--- 10000\n",
      "--- 15000\n",
      "20000 가족여행\n",
      "--- 20000\n",
      "--- 25000\n",
      "30000 영화산업\n",
      "--- 30000\n",
      "--- 35000\n",
      "40000 이어업\n",
      "--- 40000\n"
     ]
    }
   ],
   "source": [
    "unknown_noun = {}\n",
    "ii = 0\n",
    "for key1, val1 in whole_dic.items():\n",
    "    if key1 not in dic_keys:\n",
    "        unknown_noun[key1] = val1\n",
    "        if ii % 10000 == 0:\n",
    "            print(ii, key1)\n",
    "    if ii % 5000 == 0:\n",
    "        print('---', ii)\n",
    "    ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44966\n",
      "28218\n"
     ]
    }
   ],
   "source": [
    "print(len(whole_dic))\n",
    "import operator\n",
    "sorted_dic = sorted(unknown_noun.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(sorted_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "write obtained new nouns into user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_s = '/home/jupyter0/work/corpus/sejong_corpus_cleaner/scripts/user.dic'\n",
    "with open(f1_s, 'w') as f1:\n",
    "    for x_val in sorted_dic:\n",
    "        noun1 = x_val[0]\n",
    "        f1.write(noun1 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write obtained new nouns into Komoran dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_s = '/home/jupyter0/work/corpus/sejong_corpus_cleaner/scripts/dic2.word'\n",
    "with open(f1_s, 'a') as f1:\n",
    "    for x_val in sorted_dic:\n",
    "        noun1 = x_val[0] + '\\tNNG:' + str(int(x_val[1])) + '\\n'\n",
    "        #print(noun1)\n",
    "        f1.write(noun1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = sorted(list(filter(lambda x: len(x) > 1, nounsb.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
