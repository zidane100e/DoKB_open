{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Aug 9, 2016  \n",
    "@author: Xiangnan He (xiangnanhe@gmail.com)  \n",
    "\n",
    "Modified on Dec 7, 2018  \n",
    "@author: bwlee@kbfg.com  \n",
    "\n",
    "Keras Implementation of Multi-Layer Perceptron (MLP) recommender model in:  \n",
    "He Xiangnan et al. Neural Collaborative Filtering. In WWW 2017.   \n",
    "\n",
    "#### 수정사항\n",
    "* 전처리 과정 효율화\n",
    "    * 기존 함수 get_train_instances의 경우 item의 negative instance 랜덤 샘플을 하나씩 수행  \n",
    "      --> 각 user 별 negative instance 수집을 한 번에 전체 샘플을 선택하도록 수정\n",
    "* 기존 코드는 keras v1을 사용하여 v2 에 맞춰 수정\n",
    "* 기존 코드는 python2 를 사용하여 python3 에 맞게 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering\n",
    "상품 추천에서 최근 가장 널리 쓰이는 Collaborative Filtering(CF) 방법은 user와 item 간의 선형 곱으로 둘 간의 상호작용을 표현한다.  \n",
    "해당 상호작용 부분을 인공신경망으로 대체할 경우 둘 간의 연관성을 비선형모델로 분석 가능하다.  \n",
    "상세한 설명은 아래에서 확인 가능하다.  \n",
    "https://arxiv.org/abs/1708.05031  \n",
    "https://github.com/hexiangnan/neural_collaborative_filtering  \n",
    "\n",
    "위의 자료에서는 CF를 기존 방법인 Matrix factorization(MF) 과 MLP 방법으로 구현하고, 두 방법을 결합한 형태로 최종 결과를 구하였다.  \n",
    "MF 방법 또한 인공신경망으로 구성 가능하며 ~/samples/music_recommendation 에서 적용한 방법과 동일하다.\n",
    "아래 코드에서는 MLP 방법을 구현하였다.\n",
    "\n",
    "## Data\n",
    "MovieLens  \n",
    "영화 평가 사이트에서 데이터를 공개하고 있으면 자료 양에 따라 몇 개의 dataset 을 제공  \n",
    "이 코드는 아래 위치에서 원본 데이터를 수집 후 1차 가공한 데이터를 사용  \n",
    "https://grouplens.org/datasets/movielens/1m/  \n",
    "\n",
    "## 모델 구성\n",
    "이 모델의 구성은 아래의 그림과 같다.  \n",
    "일반적인 인공신경망과 비교해서 특징적인 점은 user와 item으로 부터 두 개의 별도 네트워크를 구성하고, 중간에서 합친다는 점이다.  \n",
    "이를 통해 전혀 다른 두 개의 특성을 연결시키고, 두 개의 상호관계를 인공신경망을 통해 추정하는 것이다.  \n",
    "<img src=\"MLP_ncf.PNG\" width=\"600\"/>\n",
    "\n",
    "이 방법의 CF의 본질적인 문제점을 가지고 있는데, user 와 item 이 사전에 정해져 있어야 한다는 것이다. 신규 가입자와 신규 상품의 경우 정상적인 예측이 불가능하다.  \n",
    "이를 위해서는 해당 신규 항목에 대해 어느 정도 시간 동안 데이터 축적 후 이들을 새롭게 포함하여 모델을 업데이트 하는 것이다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ac6d73788a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdagrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten, Dropout, Concatenate\n",
    "#from keras.merge import Concatenate\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras import initializers\n",
    "from evaluate import evaluate_model\n",
    "from Dataset import Dataset\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal():\n",
    "    return initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "\n",
    "def get_model(num_users, num_items, layers = [20,10], reg_layers=[0,0]):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    \n",
    "    user_embed = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = 'user_embedding',\n",
    "                           embeddings_initializer = initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), input_length=1)\n",
    "    item_embed = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'item_embedding',\n",
    "                           embeddings_initializer = initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(user_embed(user_input))\n",
    "    item_latent = Flatten()(item_embed(item_input))\n",
    "    \n",
    "    # The 0-th layer is the concatenation of embedding layers\n",
    "    vector = Concatenate(1)([user_latent, item_latent])\n",
    "    \n",
    "    # MLP layers\n",
    "    for idx in range(1, num_layer):\n",
    "        #layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name = 'layer%d' %idx)\n",
    "        layer = Dense(layers[idx], activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "        \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_user_item(arr_dict, n_user, n_item):\n",
    "    ret = { i: [] for i in range(n_user) }\n",
    "    items0 = set(range(n_item))\n",
    "    rem = { i: None for i in range(n_item) }\n",
    "    for key1, _ in arr_dict.items():\n",
    "        ret[key1[0]].append(key1[1])\n",
    "    for ii in range(n_user):\n",
    "        rem[ii] = list( items0 - set(ret[ii]) )\n",
    "    return ret, rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    num_user, num_item = train.shape\n",
    "    user_input, item_input, labels = [], [], []\n",
    "        \n",
    "    # original code getting negative samples is very slow.\n",
    "    # Added hash and get random negative values first\n",
    "    \n",
    "    # arrange train data into hash with user key\n",
    "    pos_dic, neg_dic = get_dict_user_item(train, num_user, num_item)\n",
    "    \n",
    "    for u in range(num_user):\n",
    "        if u%2000 == 0:\n",
    "            print('---------u = ', u)\n",
    "        pos_items = pos_dic[u]\n",
    "        n_pos = len(pos_items)\n",
    "        if n_pos*num_negatives < len(neg_dic[u]):\n",
    "            neg_items = np.random.choice(neg_dic[u], size=(n_pos, num_negatives), replace=False)\n",
    "        else:\n",
    "            neg_items = []\n",
    "            for i in range(n_pos):\n",
    "                neg_items.append( np.random.choice(neg_dic[u], size=(num_negatives), replace=False) )\n",
    "\n",
    "        i_pos = 0\n",
    "        for i_item in pos_items:\n",
    "            # positive instance\n",
    "            user_input.append(u)\n",
    "            item_input.append(i_item)\n",
    "            labels.append(1)\n",
    "            # negative instances\n",
    "            for j_item in neg_items[i_pos]:\n",
    "                user_input.append(u)\n",
    "                item_input.append(j_item)\n",
    "                labels.append(0)\n",
    "            i_pos += 1\n",
    "        \n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/'\n",
    "dataset = 'ml-1m'\n",
    "layers = [64, 32, 16, 8]\n",
    "reg_layers = [0, 0, 0, 0]\n",
    "num_negatives = 4\n",
    "learner = 'adam'\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "verbose = 1\n",
    "    \n",
    "topK = 10\n",
    "evaluation_threads = 1 #mp.cpu_count()\n",
    "model_out_file = 'Pretrain/%s_MLP_%s_%d.h5' %(dataset, layers, time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [18.4 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "t1 = time()\n",
    "dataset = Dataset(path + dataset)\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "        %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = get_model(num_users, num_items, layers, reg_layers)\n",
    "if learner.lower() == \"adagrad\": \n",
    "    model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"adam\":\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "else:\n",
    "    model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.1060, NDCG = 0.0493 [54.2]\n"
     ]
    }
   ],
   "source": [
    "# Check Init performance\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f [%.1f]' %(hr, ndcg, time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 0\n",
      "--------------------- fit end 0\n",
      "Iteration 0 [582.6 s]: HR = 0.5858, NDCG = 0.3330, loss = 0.3032 [54.4 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 1\n",
      "--------------------- fit end 1\n",
      "Iteration 1 [583.7 s]: HR = 0.6106, NDCG = 0.3493, loss = 0.2837 [53.7 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 2\n",
      "--------------------- fit end 2\n",
      "Iteration 2 [588.0 s]: HR = 0.6280, NDCG = 0.3605, loss = 0.2731 [54.5 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 3\n",
      "--------------------- fit end 3\n",
      "Iteration 3 [590.4 s]: HR = 0.6397, NDCG = 0.3679, loss = 0.2663 [54.3 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 4\n",
      "--------------------- fit end 4\n",
      "Iteration 4 [599.2 s]: HR = 0.6432, NDCG = 0.3736, loss = 0.2616 [54.4 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 5\n",
      "--------------------- fit end 5\n",
      "Iteration 5 [594.7 s]: HR = 0.6492, NDCG = 0.3811, loss = 0.2576 [55.1 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 6\n",
      "--------------------- fit end 6\n",
      "Iteration 6 [608.7 s]: HR = 0.6555, NDCG = 0.3813, loss = 0.2547 [54.8 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 7\n",
      "--------------------- fit end 7\n",
      "Iteration 7 [608.3 s]: HR = 0.6629, NDCG = 0.3879, loss = 0.2523 [54.8 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 8\n",
      "--------------------- fit end 8\n",
      "Iteration 8 [609.6 s]: HR = 0.6647, NDCG = 0.3886, loss = 0.2499 [54.7 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 9\n",
      "--------------------- fit end 9\n",
      "Iteration 9 [607.1 s]: HR = 0.6581, NDCG = 0.3829, loss = 0.2480 [54.4 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 10\n",
      "--------------------- fit end 10\n",
      "Iteration 10 [604.6 s]: HR = 0.6644, NDCG = 0.3906, loss = 0.2464 [54.5 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 11\n",
      "--------------------- fit end 11\n",
      "Iteration 11 [604.4 s]: HR = 0.6586, NDCG = 0.3871, loss = 0.2449 [54.3 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 12\n",
      "--------------------- fit end 12\n",
      "Iteration 12 [605.4 s]: HR = 0.6632, NDCG = 0.3905, loss = 0.2435 [54.6 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 13\n",
      "--------------------- fit end 13\n",
      "Iteration 13 [607.0 s]: HR = 0.6631, NDCG = 0.3922, loss = 0.2425 [54.7 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 14\n",
      "--------------------- fit end 14\n",
      "Iteration 14 [606.5 s]: HR = 0.6616, NDCG = 0.3894, loss = 0.2413 [54.8 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 15\n",
      "--------------------- fit end 15\n",
      "Iteration 15 [598.2 s]: HR = 0.6624, NDCG = 0.3879, loss = 0.2404 [54.7 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 16\n",
      "--------------------- fit end 16\n",
      "Iteration 16 [592.4 s]: HR = 0.6646, NDCG = 0.3925, loss = 0.2396 [54.2 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 17\n",
      "--------------------- fit end 17\n",
      "Iteration 17 [585.4 s]: HR = 0.6634, NDCG = 0.3908, loss = 0.2387 [53.9 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 18\n",
      "--------------------- fit end 18\n",
      "Iteration 18 [587.2 s]: HR = 0.6641, NDCG = 0.3901, loss = 0.2379 [54.6 s]\n",
      "---------u =  0\n",
      "---------u =  2000\n",
      "---------u =  4000\n",
      "---------u =  6000\n",
      "--------------------- fit start 19\n",
      "--------------------- fit end 19\n",
      "Iteration 19 [587.9 s]: HR = 0.6671, NDCG = 0.3953, loss = 0.2372 [54.0 s]\n",
      "End. Best Iteration 19:  HR = 0.6671, NDCG = 0.3953. \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-68286c49829e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ndcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best MLP model is saved to %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    print('--------------------- fit start', epoch)\n",
    "    # Training        \n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                    np.array(labels), # labels \n",
    "                    batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "    t2 = time()\n",
    "    print('--------------------- fit end', epoch)\n",
    "    # Evaluation\n",
    "    if epoch %verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "            % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "if args.out > 0:\n",
    "    print(\"The best MLP model is saved to %s\" %(model_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
